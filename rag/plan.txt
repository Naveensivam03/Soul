 we need to convert the user input into vector embeddings anf then store it in the vector database .

 use that as the rag file to learn for the ai.


 based on all the conversations the ai will generate an daily text summarization and provide that to the user whenever teh user feels and also the data fo the user is categorized based on emotions and sentiments and date.


 

 tommorrows task:

 -[] convert teh user speech and categories it based on the user emotions and sentiments.
 -[] convert those into embeddings and then we can able to store that in a vector database.
 -[] and also generate a summary based on the user speech until that day.



--pipeline--
1: user input from the whisper is passed into two models to find out the sentiment and embeddings to convert that and stored that into the database.

MODELS:

    EMBEDDINGS:
        - thenlper/gte-small
    SENTIMENT :
        - j-hartmann/emotion-english-distilroberta-base

2: once the user text is converted into the embeddings store that into the chromadb.
3: instead of storing it into the db chunks wise lets assume each speech as single chunks and implement that.





